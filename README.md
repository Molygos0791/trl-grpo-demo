# 基于 GRPO 的大语言模型对齐实验（TRL）

本项目基于 **Hugging Face TRL** 框架，实现了一个完整的  
**Group Relative Policy Optimization（GRPO）** 大语言模型对齐训练流程。

项目结果可能不是很好看，but本项目 
在于 **LLM 强化学习训练过程、reward 设计以及模型行为分析**，  
旨在复现并理解真实 LLM 对齐中的关键问题。

---

## 一、项目背景

在大语言模型（LLM）中，**监督微调（SFT）** 可以让模型学会“如何回答”，  
但难以精细控制模型在 **多种合理回答之间的偏好分布**。

，**基于强化学习的对齐方法（RLHF）** 被广泛用于模型行为调控。  
其中，**Group Relative Policy Optimization（GRPO）** 具有以下特点：

- 通过 **组内相对奖励** 更新策略
- 不依赖显式的 value model
- 相比 PPO 具有更低的实现复杂度和更稳定的训练行为

本项目在中文指令模型上完整实现 GRPO，并对训练行为进行分析。

---

## 二、方法概述

### 1. GRPO 训练流程

对于每一个 prompt，训练过程如下：

1. 模型基于当前策略 **生成多条候选回答（group sampling）**
2. 使用 **规则化 reward 函数** 对回答进行打分
3. 在同一 prompt 的回答组内计算 **相对优势**
4. 更新模型策略，使其更偏向高 reward 的生成行为

与 PPO 不同，GRPO **不需要训练 value network**，  
从而简化了整体训练结构。

---

### 2. 训练数据设计

- 训练数据 **仅包含 prompt**
- 所有回答均由模型在训练过程中 **在线生成**
- 不依赖人工标注的标准答案

示例 prompt：
请给我一个三点式学习计划，用 '-' 做 bullet，
每一条都必须包含一个时间（如 30 分钟），
不要有多余解释，只输出三条 bullet。


---

### 3. Reward 函数设计

本项目采用 **基于规则的 reward 设计**，主要包括：

- **结构约束**：是否严格使用 `-` 作为 bullet，且数量为三条
- **内容约束**：每条 bullet 是否包含时间表达（如“30分钟”“1小时”）
-无硬约束

该 reward 设计具有良好的 **可解释性与可调试性**，  
并用于分析强化学习中常见的 **reward hacking** 问题。（他确实发生了ToT）

---

## 三、实验设置

- **模型**：Qwen/Qwen2.5-0.5B-Instruct  
- **算法**：GRPO（TRL 实现）  
- **Group 大小**：每个 prompt 生成 4 条回答  
- **训练方式**：在线强化学习（prompt-only）  
- **运行环境**：Apple Silicon（MPS / CPU）

---

## 四、评估方式

本项目不以单次输出作为评估标准，而是采用：

- 训练前 / 训练后的 **多次采样对比**
- 观察生成结果中 **合规行为出现的概率变化**
- 对训练过程中出现的退化现象进行定性分析

该评估方式符合强化学习中 **分布层面分析** 的基本原则。

---

## 五、实验观察与分析

在实验过程中观察到多种真实 RLHF 现象，包括：

- 模型通过复述 prompt 或生成无意义内容来规避 reward（reward hacking）
- 训练行为对 reward 设计与采样温度高度敏感
- 引入硬约束与 reward shaping 后，模型行为逐步稳定

上述现象与已有 LLM 对齐研究中的结论保持一致。

---

## 六、项目结构

```text
.
├── train_grpo.py        # GRPO 训练与评估脚本
├── requirements.txt     # 项目依赖
├── README.md
└── .gitignore



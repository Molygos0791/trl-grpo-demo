# GRPO-based LLM Alignment Demo

This project demonstrates a minimal **LLM alignment experiment** using  
**Group Relative Policy Optimization (GRPO)** with **Hugging Face TRL**.

- Model: Qwen/Qwen2.5-0.5B-Instruct
- Algorithm: GRPO (no value model)
- Training style: prompt-only online RL
- Reward: rule-based (structure + content constraints)

## How to run

```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
python train_grpo.py
